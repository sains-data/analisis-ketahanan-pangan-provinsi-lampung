# Lampung Food Security Big Data System - Environment Configuration Template
# Copy this file to .env and update the values according to your environment

# =============================================================================
# ENVIRONMENT SETTINGS
# =============================================================================
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO
PYTHONPATH=/opt/project/src

# =============================================================================
# HADOOP & HDFS CONFIGURATION
# =============================================================================
HADOOP_HOME=/opt/hadoop
HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
HDFS_NAMENODE_HOST=namenode
HDFS_NAMENODE_PORT=9000
HDFS_BASE_URL=hdfs://namenode:9000

# Data paths in HDFS
HDFS_BRONZE_PATH=/data/bronze
HDFS_SILVER_PATH=/data/silver
HDFS_GOLD_PATH=/data/gold
HDFS_CHECKPOINT_PATH=/tmp/checkpoints

# =============================================================================
# SPARK CONFIGURATION
# =============================================================================
SPARK_HOME=/spark
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_WAREHOUSE_DIR=hdfs://namenode:9000/user/hive/warehouse
SPARK_DRIVER_MEMORY=2g
SPARK_EXECUTOR_MEMORY=2g
SPARK_EXECUTOR_CORES=2
SPARK_MAX_RESULT_SIZE=1g
SPARK_SERIALIZER=org.apache.spark.serializer.KryoSerializer

# =============================================================================
# HIVE CONFIGURATION
# =============================================================================
HIVE_HOST=hiveserver2
HIVE_PORT=10000
HIVE_JDBC_URL=jdbc:hive2://hiveserver2:10000
HIVE_METASTORE_HOST=hive-metastore
HIVE_METASTORE_PORT=9083
HIVE_WAREHOUSE_DIR=hdfs://namenode:9000/user/hive/warehouse

# Database names
HIVE_BRONZE_DB=bronze
HIVE_SILVER_DB=silver
HIVE_GOLD_DB=gold

# =============================================================================
# AIRFLOW CONFIGURATION
# =============================================================================
AIRFLOW_HOME=/opt/airflow
AIRFLOW_HOST=airflow-webserver
AIRFLOW_PORT=8085
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_EXECUTOR=LocalExecutor

# Airflow Database Configuration
AIRFLOW_DB_NAME=airflow
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow123

# Airflow Security Keys
AIRFLOW_FERNET_KEY=UKMzEm3yIuFYEq1y3-2FxPNWSVwRASpahmQ9kQfEr8=
AIRFLOW_SECRET_KEY=lampung-food-security-key

# DAG Configuration
AIRFLOW_DAG_DIR=/opt/airflow/dags
AIRFLOW_SCHEDULE_INTERVAL=@daily
AIRFLOW_MAX_ACTIVE_RUNS=1
AIRFLOW_CATCHUP=false

# =============================================================================
# SUPERSET CONFIGURATION
# =============================================================================
SUPERSET_HOST=superset
SUPERSET_PORT=8090
SUPERSET_ADMIN_USER=admin
SUPERSET_ADMIN_PASSWORD=admin123
SUPERSET_SECRET_KEY=your-superset-secret-key-change-this-in-production

# Database URI for Superset
SUPERSET_DATABASE_URI=hive://hive@hiveserver2:10000/gold

# =============================================================================
# DATA PROCESSING CONFIGURATION
# =============================================================================
# CSV Processing
CSV_SEPARATOR=,
CSV_ENCODING=utf-8
CSV_HEADER=true
INFER_SCHEMA=true

# Partitioning
PARTITION_COLUMN=tahun
OUTPUT_FORMAT=parquet
WRITE_MODE=overwrite

# Data Quality Thresholds
COMPLETENESS_THRESHOLD=90
VALIDITY_THRESHOLD=95
UNIQUENESS_THRESHOLD=98

# =============================================================================
# PIPELINE CONFIGURATION
# =============================================================================
PIPELINE_BATCH_SIZE=10000
PIPELINE_MAX_RETRIES=3
PIPELINE_RETRY_DELAY=60
PIPELINE_TIMEOUT=3600
PIPELINE_CHECKPOINT_ENABLED=true

# Data Generation (for testing)
SAMPLE_DATA_ROWS=500000
SAMPLE_DATA_YEARS=10

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
LOG_FORMAT="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
LOG_FILE=/opt/project/logs/pipeline.log
LOG_ROTATION=daily
LOG_RETENTION_DAYS=30

# Spark Logging
SPARK_LOG_LEVEL=WARN
HADOOP_LOG_LEVEL=WARN

# =============================================================================
# MONITORING & PERFORMANCE
# =============================================================================
# Memory and Performance
MAX_MEMORY_USAGE=8g
CACHE_ENABLED=true
METRICS_ENABLED=true
PROFILING_ENABLED=false

# Health Check Endpoints
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=10

# =============================================================================
# FILE PATHS
# =============================================================================
PROJECT_ROOT=/opt/project
DATA_DIR=/opt/project/data
SCRIPTS_DIR=/opt/project/scripts
CONFIGS_DIR=/opt/project/configs
LOGS_DIR=/opt/project/logs

# =============================================================================
# EXTERNAL SERVICES (if applicable)
# =============================================================================
# MySQL/PostgreSQL for metadata (if used)
METASTORE_DB_HOST=localhost
METASTORE_DB_PORT=3306
METASTORE_DB_NAME=metastore
METASTORE_DB_USER=hive
METASTORE_DB_PASSWORD=hive123

# S3 or Object Storage (if used)
S3_ENDPOINT=
S3_ACCESS_KEY=
S3_SECRET_KEY=
S3_BUCKET=

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
# Authentication
AUTH_ENABLED=false
KERBEROS_ENABLED=false
SSL_ENABLED=false

# Encryption
ENCRYPT_DATA_AT_REST=false
ENCRYPT_DATA_IN_TRANSIT=false

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================
# Testing
RUN_TESTS=false
TEST_DATA_SIZE=1000
MOCK_EXTERNAL_SERVICES=true

# Debugging
ENABLE_DEBUG_MODE=false
PROFILE_PERFORMANCE=false
VALIDATE_SCHEMAS=true

# Docker Compose overrides
COMPOSE_PROJECT_NAME=lampung-food-security
COMPOSE_FILE=docker-compose.yml
